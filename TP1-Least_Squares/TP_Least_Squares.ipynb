{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer lab :\n",
    "\n",
    "Group Members : Ayoub Marzoug - Adnane El Bouhali\n",
    "\n",
    "## Let’s reverse-engineer the data center SD-TSIA 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1 : Show that if $Aw=b$, then $\\displaystyle y(t)=\\frac{w_1^{\\top} \\tilde{x}(t)+w_0}{w_2^{\\top} \\tilde{x}(t)+1}.$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $Aw=b$, then for all $t$ :\n",
    "$$ (Aw)_t = \\tilde{x}(t)^{T}w_1 + w_0 − y(t)\\times \\tilde{x}(t)^{T}w_2 = y(t)$$\n",
    "\n",
    "$$y(t)\\big(w_2^{T}\\tilde{x}(t)+ 1\\big) = w_1^{T}\\tilde{x}(t) + w_0$$\n",
    "\n",
    "$$y(t) = \\frac{w_1^{T}\\tilde{x}(t) + w_0}{w_2^{T}\\tilde{x}(t)+ 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2 : Solve this least squares problem using the function `numpy.linalg.lstsq`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmin = np.linalg.lstsq(A, b, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3 : Evaluate the quality of the solution found on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrices for the test set\n",
    "\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.769461864685916e-28 780.898479352361\n"
     ]
    }
   ],
   "source": [
    "# Predicting the values\n",
    "y_pred = np.dot(A_test, wmin)\n",
    "\n",
    "# Calculating the mean squared error\n",
    "mse_train = np.mean((np.dot(A, wmin) - b)**2)\n",
    "mse_test = np.mean((y_pred - b_test)**2)\n",
    "print(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 : In order to improve the generalization power of the model, we consider a $\\mathscr l_2$ regularization : $\\min_w\\frac 12\\|Aw − b\\|^2 +\\frac λ2 \\|w\\|^2$ where $λ = 100$. Solve this problem and compare the test mean square error with the unregularized one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment A and b\n",
    "λ = 100\n",
    "n_col = A.shape[1]\n",
    "A_augmented = np.vstack([A, np.sqrt(λ) * np.identity(n_col)])\n",
    "b_augmented = np.vstack([b.reshape(-1, 1), np.zeros((n_col, 1))])\n",
    "\n",
    "# Solve the modified least squares problem\n",
    "wridge = np.linalg.lstsq(A_augmented, b_augmented, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.171430604344522 316.9616528129709\n"
     ]
    }
   ],
   "source": [
    "# Predicting the values\n",
    "y_pred = np.dot(A_test, wridge)\n",
    "\n",
    "# Calculating the mean squared error\n",
    "mse_train = np.mean((np.dot(A, wridge) - b)**2)\n",
    "mse_test = np.mean((y_pred - b_test)**2)\n",
    "print(mse_train, mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5 : Calculate the gradient of $f_1 : w\\mapsto \\frac 12 \\|Aw − b\\|^2 +\\frac λ2 \\|w\\|^2$ . Is the function convex ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $w \\in \\mathbb{R}^{2d+1}$, we have :\n",
    "$$\n",
    "f_1(w) = \\frac{1}{2} (Aw - b)^{T}(Aw-b) + \\frac{\\lambda}{2} w^{T}w\n",
    "$$\n",
    "Thus :\n",
    "$$\n",
    "\\nabla f_1(w) = \\frac{1}{2}\\big(A^{T}(Aw-b)+A^{T}(Aw-b)\\big) + \\frac{\\lambda}2\\big(w + w \\big)\n",
    "\\\\ = A^{T}(Aw-b) + \\lambda w\n",
    "$$\n",
    "$$\n",
    "\\nabla^2 f_1(w) = A^{T}A + \\lambda I\n",
    "$$\n",
    "This function is convex, since the $\\lambda I$ is positive definite for any $\\lambda>0$, and $A^𝑇A$ is positive semi-definite for any $A$. The sum of a positive definite and positive semi-definite matrix is positive definite, QED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6 : Implement gradient descent to minimize $f_1$. What step size are you choosing? How many iterations are needed to get $w_k$ such that $\\|∇f (w_k)\\| ≤ 1$ ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(A, b, w, λ):\n",
    "    term1 = np.linalg.norm(A @ w - b)**2 / 2\n",
    "    term2 = λ * np.linalg.norm(w)**2 / 2\n",
    "    return term1 + term2\n",
    "\n",
    "def gradient_f1(A, b, w, λ):\n",
    "    A = np.array(A)\n",
    "    return A.T @ (A @ w - b) + λ * w\n",
    "\n",
    "def gradient_descent(A, b, λ, initial_w, alpha):\n",
    "    w = initial_w\n",
    "    num_iterations = 0\n",
    "    while np.linalg.norm(gradient_f1(A, b, w, λ)) > 1:\n",
    "        gradient = gradient_f1(A, b, w, λ)\n",
    "        w = w - alpha * gradient\n",
    "        num_iterations += 1\n",
    "    return num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(A, b, λ, initial_w, alpha_values):\n",
    "    best_alpha = None\n",
    "    min_iterations = float('inf')\n",
    "\n",
    "    for alpha in alpha_values:\n",
    "        iterations = gradient_descent(A, b, λ, initial_w, alpha)\n",
    "        if iterations < min_iterations:\n",
    "            min_iterations = iterations\n",
    "            best_alpha = alpha\n",
    "\n",
    "    return best_alpha, min_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/335825371.py:8: RuntimeWarning: overflow encountered in matmul\n",
      "  return A.T @ (A @ w - b) + λ * w\n",
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/335825371.py:8: RuntimeWarning: invalid value encountered in matmul\n",
      "  return A.T @ (A @ w - b) + λ * w\n",
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/335825371.py:8: RuntimeWarning: overflow encountered in multiply\n",
      "  return A.T @ (A @ w - b) + λ * w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 2.0\n",
      "Minimum Iterations: 45\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(1785)\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "best_alpha, min_iterations = grid_search(A, b, λ, initial_w, alpha_values)\n",
    "\n",
    "print(f\"Best Alpha: {best_alpha}\")\n",
    "print(f\"Minimum Iterations: {min_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7 : (Bonus question) Implement the conjugate gradient method. Compare the convergence rate with the previous algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=1000):\n",
    "    n = len(b)\n",
    "    \n",
    "    if x0 is None:\n",
    "        x0 = np.zeros(n)\n",
    "    \n",
    "    x = x0\n",
    "    r = b - np.dot(A, x)\n",
    "    p = r\n",
    "    rsold = np.dot(r, r)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        Ap = np.dot(A.T, p)  # Transpose A before multiplication\n",
    "        alpha = rsold / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r = r - alpha * np.dot(A, p)\n",
    "        rsnew = np.dot(r, r)\n",
    "        if np.sqrt(rsnew) < tol:\n",
    "            break\n",
    "        beta = rsnew / rsold\n",
    "        p = r + beta * p\n",
    "        rsold = rsnew\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization for a sparse model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 : Write the objective function as $F_2 = f_2 + g_2$ where $f_2$ is differentiable and the proximal operator of $g_2$ is easy to compute. Recall the formula for $prox_{g2}$. Calculate the gradient of $f_2$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is : $F_2(w) = \\frac{1}{2}\\|Aw-b\\|^2 + \\lambda \\|w\\|_1$.\n",
    "\n",
    "We have $f_2(w) = \\frac{1}{2}\\|Aw-b\\|^2$ and $g_2(w) = \\lambda \\|w\\|_1$.\n",
    "\n",
    "The proximal operator of $g_2$ is easy to compute since it is the soft thresholding operator : $prox_{g2}(w) = S_{\\lambda}(w)$.\n",
    "\n",
    "The gradient of $f_2$ is :\n",
    "\n",
    "$$\n",
    "\\nabla f_2(w) = A^{T}(Aw-b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2 : Code the proximal gradient method. Here, we will take $λ = 200$. What stopping test do you suggest ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholding(x, threshold):\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "def proximal_gradient(A, b, λ, initial_w, alpha, max_iter=1000, tol=1e-6):\n",
    "    w = initial_w\n",
    "    for i in range(max_iter):\n",
    "        gradient = A.T @ (A @ w - b)\n",
    "        w_new = soft_thresholding(w - alpha * gradient, λ * alpha)\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            break\n",
    "        w = w_new\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlasso = proximal_gradient(A, b, 200, initial_w, alpha=0.0001, max_iter=100, tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.3 : We may try to accelerate the algorithm using line search. Compare the speed of the algorithm with fixed step size and with line search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_proximal_gradient(A, b, λ, initial_w, max_iter=1000, tol=1e-6):\n",
    "    w = initial_w\n",
    "    alpha = 1.0\n",
    "    for i in range(max_iter):\n",
    "        gradient = A.T @ (A @ w - b)\n",
    "        w_new = soft_thresholding(w - alpha * gradient, λ * alpha)\n",
    "        obj_new = 0.5 * np.linalg.norm(A @ w_new - b)**2 + λ * np.linalg.norm(w_new, 1)\n",
    "        obj_diff = obj_new - (0.5 * np.linalg.norm(A @ w - b)**2 + λ * np.linalg.norm(w, 1))\n",
    "        while obj_diff > 0:\n",
    "            alpha *= 0.5\n",
    "            w_new = soft_thresholding(w - alpha * gradient, λ * alpha)\n",
    "            obj_new = 0.5 * np.linalg.norm(A @ w_new - b)**2 + λ * np.linalg.norm(w_new, 1)\n",
    "            obj_diff = obj_new - (0.5 * np.linalg.norm(A @ w - b)**2 + λ * np.linalg.norm(w, 1))\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            break\n",
    "        w = w_new\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/3118583458.py:7: RuntimeWarning: overflow encountered in matmul\n",
      "  gradient = A.T @ (A @ w - b)\n",
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/3118583458.py:7: RuntimeWarning: invalid value encountered in matmul\n",
      "  gradient = A.T @ (A @ w - b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7472579479217529, 1.7163522243499756)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Fixed step size\n",
    "start_time_fixed = time.time()\n",
    "w_fixed = proximal_gradient(A, b, 200, initial_w, alpha=0.1, max_iter=1000, tol=1e-6)\n",
    "end_time_fixed = time.time()\n",
    "execution_time_fixed = end_time_fixed - start_time_fixed\n",
    "\n",
    "# Line search\n",
    "start_time_line_search = time.time()\n",
    "w_line_search = line_search_proximal_gradient(A, b, 200, initial_w, max_iter=1000, tol=1e-6)\n",
    "end_time_line_search = time.time()\n",
    "execution_time_line_search = end_time_line_search - start_time_line_search\n",
    "\n",
    "execution_time_fixed, execution_time_line_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.1 : Compare the solutions obtained by the two types of regularization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1v/y6b4xp493972fjd99f2w41j80000gn/T/ipykernel_10649/2798786856.py:5: RuntimeWarning: overflow encountered in square\n",
      "  mse_lasso = np.mean((np.dot(A_test, wlasso) - b_test)**2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(316.9616528129709, inf)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate MSE for unregularized solution\n",
    "mse_ridge = np.mean((np.dot(A_test, wridge) - b_test)**2)\n",
    "\n",
    "# Calculate MSE for regularized solution\n",
    "mse_lasso = np.mean((np.dot(A_test, wlasso) - b_test)**2)\n",
    "\n",
    "mse_ridge, mse_lasso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
